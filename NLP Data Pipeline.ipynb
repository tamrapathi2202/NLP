{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This Jupyter Notebook introduces key concepts in text preprocessing and sentiment analysis using Natural Language Processing (NLP) techniques. It leverages Python libraries like NLTK and scikit-learn to demonstrate various NLP tasks and build a basic sentiment classification model. The notebook covers text tokenization at both the word and sentence levels, providing practical examples of processing English sentences and handling input text. It explores the removal of stopwords, with examples in English and French, and demonstrates how to view and work with predefined lists of stopwords in NLTK. The workflow for building a sentiment analysis model involves vectorizing text data with CountVectorizer, splitting datasets into training and testing subsets with varying train-test ratios, and training a Naive Bayes classifier using scikit-learn's MultinomialNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Stages of the Notebook\n",
    "<b>1. Importing the necessary Libraries</b><br>\n",
    "Essential libraries for this implementation are:\n",
    "- NLTK: A comprehensive library for working with human language data, providing tools for text processing, tokenization, stemming, lemmatization, and more.\n",
    "- Scikit-learn: A versatile machine learning library for building, training, and evaluating models, with support for classification, regression, clustering, and preprocessing tasks.\n",
    "\n",
    "<b>2. Text Pre-Processing</b><br>\n",
    "- *Word Tokenization*: Splitting a text into individual words for analysis.\n",
    "- *Sentence Tokenization*: Dividing a text into individual sentences for processing.\n",
    "- *Lower Casing*: Converting all characters in a text to lowercase to ensure uniformity.\n",
    "- *Stopwords Removal*: Eliminating commonly used words (e.g., \"the\", \"and\") that add little meaning to text analysis.\n",
    "- *Stemming*: Reducing words to their root form by removing suffixes.\n",
    "- *Lemmatization*: Reducing words to their base or dictionary form based on linguistic rules.\n",
    "\n",
    "<b>3. Splitting the Train and Test Data</b><br> \n",
    "The purpose of splitting a dataset into training and testing subsets is to evaluate the performance of a machine learning model on unseen data. This ensures that the model performs well not only on the data it was trained on but also on new, unseen data, reducing the risk of overfitting and providing a more reliable estimate of its real-world performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Outcome\n",
    "Upon completion of this Notebook, students will be able to:\n",
    "- Learn to tokenize text into words and sentences using NLTK for further text analysis.\n",
    "- Gain the ability to identify and remove stopwords in multiple languages to improve text processing.\n",
    "- Develop skills in text normalization techniques such as stemming and lemmatization to standardize textual data.\n",
    "- Learn to convert text into numerical features using CountVectorizer.\n",
    "- Gain experience in training and testing a sentiment analysis model using the Naive Bayes algorithm with scikit-learn.\n",
    "- Explore the impact of varying train-test splits on model performance to understand model generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What kind of AI Projects would this Jupyter Notebook extend to?\n",
    "This Jupyter Notebook (JN) can be extended to a variety of AI projects involving text data and Natural Language Processing (NLP):\n",
    "- Language Translation: Integrate tokenization, stemming, and lemmatization with translation APIs or models to preprocess and translate text efficiently.\n",
    "- Social Media Analytics: Analyze Twitter or Facebook data for trends, opinions, and sentiment using preprocessing and classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Flow\n",
    "An outline of the tasks performed in this Python implementation:\n",
    "1. [Import the Libraries](#import-the-libraries)\n",
    "2. [Text Pre-Processing](#text-pre-processing)\n",
    "    - [Word Tokenization](#word-tokenization)\n",
    "    - [Sentence Tokenization](#sentence-tokenization)\n",
    "    - [Lower Casing](#lower-casing)\n",
    "    - [Stopwords Removal](#stopwords-removal)\n",
    "    - [Stemming](#stemming)\n",
    "    - [Lemmatization](#lemmatization)\n",
    "3. [Splitting Train and Text Data](#splitting-train-and-test-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Required\n",
    "It would take about an hour to complete the process discussed in this notebook. Follow the instructions and go through the additional explanations in this Notebook for easier execution.\n",
    "\n",
    "### Hardware Requirement:\n",
    "Any computer with access to internet and web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Libraries\n",
    "The following libraries are imported:\n",
    "- NLTK: A Python library for natural language processing, offering tools for text analysis, tokenization, and linguistic tasks.\n",
    "- PortStemmer: A stemming tool in NLTK that reduces words to their root form by removing suffixes.\n",
    "- WordNetLemmatizer: A lemmatization tool in NLTK that reduces words to their base form based on linguistic rules.\n",
    "- CountVectorizer: A scikit-learn tool that converts text data into a bag-of-words numerical representation for machine learning.\n",
    "- Train_test_split:A scikit-learn function to split datasets into training and testing subsets for model evaluation.\n",
    "- MultnomialNB: A Naive Bayes classifier in scikit-learn suitable for text data and discrete features.\n",
    "- Accuracy_score: A scikit-learn metric that calculates the ratio of correct predictions to total predictions for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary Library\n",
    "import nltk\n",
    "\n",
    "#Libraries for stopwords removal and Lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Library for Stemming and Lemmatiztion                      \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer  \n",
    "\n",
    "#Libraries for modeling and data split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', 'us', 'talk', 'about', 'Python']\n"
     ]
    }
   ],
   "source": [
    "#Input text\n",
    "data = \"Let us talk about Python\"\n",
    "\n",
    "# Word Tokenization\n",
    "nltk_tokens = nltk.word_tokenize(data)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input text\n",
    "sentence_data = \"Let's talk about Python. Let's not talk about Python.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's talk about Python.\", \"Let's not talk about Python.\"]\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'books are on the table.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Books are on the table.\"\n",
    "sentence = sentence.lower()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "The total number of stopwords in English:  198\n"
     ]
    }
   ],
   "source": [
    "#To view all the stopwords in English\n",
    "print(stopwords.words('english'))\n",
    "print(\"The total number of stopwords in English: \",len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary, we can view all the stopwords in any other language.\n",
    "Let's try this out with French!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
      "The total number of stopwords in French:  157\n"
     ]
    }
   ],
   "source": [
    "#To view all the stopwords in French\n",
    "print(stopwords.words('french'))\n",
    "print(\"The total number of stopwords in French: \",len(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's continue with removing stopwords in a given input text in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Text\n",
    "sentence = \"He is the only person in the library\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'is', 'the', 'only', 'person', 'in', 'the', 'library']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word Tokenization \n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(sentence)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'person', 'library']\n"
     ]
    }
   ],
   "source": [
    "#Removing Stopwords\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its root or base form by removing prefixes, suffixes, or other inflections. It focuses on chopping off word endings using predefined rules without considering the meaning of the word. \n",
    "\n",
    "PorterStemmer is an algorithm for stemming, which is the process of reducing words to their root or base form by removing suffixes and other word endings. It is implemented in Python through the Natural Language Toolkit (NLTK) library.\n",
    "\n",
    "We begin by initiating an instance of PortStemmer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate an instance of PortStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Text\n",
    "sentence = \"cats mice learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "mice\n",
      "learn\n"
     ]
    }
   ],
   "source": [
    "for word in sentence.split():\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe, how the words are reduced to the its base form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fyzan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fyzan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Text\n",
    "sentence = \"cats mice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'mice']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the sentence into words\n",
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'mouse']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the difference in how Stemming and Lemmatization handles words like 'mice'. <br>\n",
    "On stemming the word 'mice', the same word was returned as output. <br>\n",
    "But on lemmatizing the word 'mice', the output is a grammatically correct root word 'mouse'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Train and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and test subsets helps enhance the model's performance by ensuring that it is evaluated on unseen data. Training on the entire dataset can cause the model to memorize the data, reducing its ability to generalize. A separate test set helps assess how well the model performs on unseen examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with different training and test data split and train a sample data with Naïve Bayes algorithm. The Naive Bayes algorithm is a probabilistic machine learning algorithm primarily used for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data and corresponding labels (0: Negative, 1: Positive)\n",
    "texts = [\n",
    "    \"I love this product!\", \"This is the best thing ever!\", \n",
    "    \"Absolutely amazing experience.\", \"Not worth the price.\", \n",
    "    \"Terrible customer service.\", \"I hate this so much.\", \n",
    "    \"Would buy again.\", \"Highly recommended!\", \n",
    "    \"Waste of money.\", \"Awful quality!\"\n",
    "]\n",
    "labels = [1, 1, 1, 0, 0, 0, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to numerical features using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following ratios of \n",
    "- 90:10\n",
    "- 80:20\n",
    "- 70:30\n",
    "- 60:40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different train-test splits\n",
    "splits = [0.9, 0.8, 0.7, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split: 90-10, Test Accuracy: 1.00\n",
      "Train-Test Split: 80-20, Test Accuracy: 0.00\n",
      "Train-Test Split: 70-30, Test Accuracy: 0.25\n",
      "Train-Test Split: 60-40, Test Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=1-split, random_state=42)\n",
    "    \n",
    "    # Train a Naive Bayes model\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Train-Test Split: {split*100:.0f}-{(1-split)*100:.0f}, Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different train and test split enable the model to attain different levels of accuracy. <br>\n",
    "For the sample text data considered for this implementation, the split of 90:10 attains 100% accuracy. <br>\n",
    "The other splits considered does not produce desirable accuracy rates.<br>\n",
    "It is also important to note that this split ratio might work differntly for a different dataset.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model with new text sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sentence: This product is not worth the money!\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model with a single sentence\n",
    "test_sentence = \"This product is not worth the money!\"\n",
    "#Uncomment the following line to test the model with a different input.\n",
    "#test_sentence = \"This product awesome!\"\n",
    "\n",
    "# Transform the test sentence using the trained vectorizer\n",
    "test_features = vectorizer.transform([test_sentence])\n",
    "\n",
    "# Predict the sentiment\n",
    "predicted_label = model.predict(test_features)[0]\n",
    "\n",
    "# Map label to sentiment\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "predicted_sentiment = label_map[predicted_label]\n",
    "\n",
    "# Display the prediction\n",
    "print(f\"Test Sentence: {test_sentence}\\nPredicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "- This notebook demonstrates essential tasks of text preprocessing such as tokenization, stopword removal, stemming, and lemmatization using NLTK.\n",
    "- Text data is vectorized using CountVectorizer, transforming text into a bag-of-words representation suitable for modeling.\n",
    "- A Naive Bayes classifier (MultinomialNB) is trained and evaluated for sentiment classification using sample labeled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
